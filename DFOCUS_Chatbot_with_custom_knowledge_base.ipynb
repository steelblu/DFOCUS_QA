{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaLK4kD1Uar"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "This notebook has all the code you need to create your own chatbot with custom knowledge base using GPT-3. \n",
        "\n",
        "Follow the instructions for each steps and then run the code sample. In order to run the code, you need to press \"play\" button near each code sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0U7D-ChF-Jx"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rD4Qzglp3J-h"
      },
      "source": [
        "#Download the data for your custom knowledge base\n",
        "For the demonstration purposes we are going to use ----- as our knowledge base. You can download them to your local folder from the github repository by running the code below.ddd\n",
        "Alternatively, you can put your own custom data into the local folder. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cCyU-vV5Yb0",
        "outputId": "a8bdd29a-c865-4955-916f-9c37955e51eb"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/steelblu/DFOCUS_QA.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiUyHP4T2g5F"
      },
      "source": [
        "# Install the dependicies\n",
        "Run the code below to install the depencies we need for our functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LL4rxT6_W7h"
      },
      "outputs": [],
      "source": [
        "%pip install gpt-index\n",
        "%pip install langchain\n",
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbuYetOy25eM"
      },
      "source": [
        "# Define the functions\n",
        "The following code defines the functions we need to construct the index and query it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UelAqQgk_yIt"
      },
      "outputs": [],
      "source": [
        "from gpt_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
        "from langchain import OpenAI\n",
        "import sys\n",
        "import os\n",
        "from IPython.display import Markdown, display\n",
        "import builtins\n",
        "\n",
        "def construct_index(directory_path):\n",
        "    # set maximum input size\n",
        "    max_input_size = 4096\n",
        "    # set number of output tokens\n",
        "    num_outputs = 300\n",
        "    # set maximum chunk overlap\n",
        "    max_chunk_overlap = 20\n",
        "    # set chunk size limit\n",
        "    chunk_size_limit = 600 \n",
        "\n",
        "    # define LLM\n",
        "    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"text-davinci-003\", max_tokens=num_outputs))\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        " \n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "    \n",
        "    index = GPTSimpleVectorIndex(\n",
        "        documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper\n",
        "    )\n",
        "\n",
        "    index.save_to_disk('index.json')\n",
        "\n",
        "    return index\n",
        "\n",
        "def ask_ai():\n",
        "    index = GPTSimpleVectorIndex.load_from_disk('index.json')\n",
        "    while True: \n",
        "        # query = input(\"What do you want to ask? \")\n",
        "        \n",
        "        query = builtins.prompt(\"아무거나 물어 보세요!\")\n",
        "        response = index.query(query, response_mode=\"compact\")\n",
        "        display(Markdown(f\"Response: <b>{response.response}</b>\"))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz1jp33jGumu"
      },
      "source": [
        "# Set OpenAI API Key\n",
        "You need an OPENAI API key to be able to run this code.\n",
        "\n",
        "If you don't have one yet, get it by [signing up](https://platform.openai.com/overview). Then click your account icon on the top right of the screen and select \"View API Keys\". Create an API key.\n",
        "\n",
        "Then run the code below and paste your API key into the text input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoJHE4fsAT3w",
        "outputId": "547befb0-7182-4589-fa37-dc902f8f4edd"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"ddd\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVrddlAL4I_v"
      },
      "source": [
        "#Construct an index\n",
        "Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API.\n",
        "\n",
        "**Notice:** running this code will cost you credits on your OpenAPI account ($0.02 for every 1,000 tokens). If you've just set up your account, the free credits that you have should be more than enough for this experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCYTE2EqBB7O",
        "outputId": "d5d6828e-c9c7-4bad-d0fb-ec73e94cdd53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
            "INFO:root:> [build_index_from_documents] Total embedding token usage: 14104 tokens\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<gpt_index.indices.vector_store.vector_indices.GPTSimpleVectorIndex at 0x215ad685c50>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "construct_index(\"DFOCUS_QA/data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipJ_gYxN5cWh"
      },
      "source": [
        "#Ask questions\n",
        "It's time to have fun and test our AI. Run the function that queries GPT and type your question into the input. \n",
        "\n",
        "If you've used the provided example data for your custom knowledge base, here are a few questions that you can ask:\n",
        "1. seedCloud는?\n",
        "2. 디포커스는?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "s_uwsPGEIGsb",
        "outputId": "52c8d26a-d80c-4fa2-ee6d-86a145afa4fc"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'builtins' has no attribute 'prompt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ask_ai()\n",
            "Cell \u001b[1;32mIn[7], line 37\u001b[0m, in \u001b[0;36mask_ai\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m index \u001b[39m=\u001b[39m GPTSimpleVectorIndex\u001b[39m.\u001b[39mload_from_disk(\u001b[39m'\u001b[39m\u001b[39mindex.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m: \n\u001b[0;32m     35\u001b[0m     \u001b[39m# query = input(\"What do you want to ask? \")\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     query \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mprompt(\u001b[39m\"\u001b[39m\u001b[39m아무거나 물어 보세요!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m     response \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mquery(query, response_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcompact\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m     display(Markdown(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse: <b>\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mresponse\u001b[39m}\u001b[39;00m\u001b[39m</b>\u001b[39m\u001b[39m\"\u001b[39m))\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'builtins' has no attribute 'prompt'"
          ]
        }
      ],
      "source": [
        "ask_ai()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "name = input(\"dddd\")\n",
        "print (name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "XiUyHP4T2g5F"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
